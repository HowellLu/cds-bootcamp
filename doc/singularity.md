# Singularity on GCP bursting

This document provides more information and background on using singularity on the GCP bursting server.
Singularity is the preferred way of setting up reproducible environments that can be transported between
GCP and Greene.

## Singularity containers

Singularity is accessed through the command line, and is available on both Greene and the GCP machines.
Singularity containers are defined through the `.sif` file format, but can be created from docker containers
and other OCI containers through the `singularity pull` command.

For example, to create the pytorch singularity container at `/scratch/wz2247/singularity/images/pytorch_21.06-py3.sif`,
I ran the following command:
```{bash}
singularity pull docker://nvcr.io/nvidia/pytorch:21.08-py3
```
Note that creating singularity containers can be very IO intensive, and can be particularly
slow on networked filesystems.

## Running singularity containers

Singularity containers may be run through the `singularity exec` command.
In general, I recommend using at least the `--no-home` argument, as well as
the `--cleanenv` arguments, to increase isolation between your container and the host.
It will also often be useful to bind the `/scratch` directory (`--bind /scratch`),
as well as bind some subdirectory of `$HOME` (e.g. `--bind $HOME/.ssh`).
On GPU machines, you will also need the `--nv` flag in order to pass-through the GPUs.
```{bash}
IMAGE=/scratch/wz2247/singularity/images/pytorch_21.06-py3.sif
singularity exec --no-home --cleanenv --nv --bind /scratch --bind $HOME/.ssh $IMAGE /bin/bash
```

## Overlays

Overlays are a powerful tool to work with and modify singularity containers.
The [documentation](https://sites.google.com/a/nyu.edu/nyu-hpc/services/Training-and-Workshops/tutorials/singularity-on-greene)
from the NYU HPC team gives a good introduction on the workflow for these containers.

As an example, I have created a series of script to convert the base Nvidia pytorch container
to a customizable container with overlays. These scripts can be run in any directory on GCP instances,
and may be found in the [lecture2/scripts](../lecture2/scripts) directory.


## Starting a container

With the overlays as generated by the previous scripts, I have also included an example script
which starts the singularity instance with recommended binds and parameters, in [start_singularity.sh](../lecture2/start_singularity.sh).

## Running VSCode inside a container

In order to run VSCode remote inside a container, we may start a SSH server (`sshd`) inside
the container. The [run_sshd.sh](../lecture2/scripts/run_sshd.sh) script contains an example
of such a script to start a SSH server inside the container, after which a VSCode instance may be
connected by specifying the correct ssh configuration file. Make sure to activat your environment
containing the `sshd` executable first (if you started your container with the `start_singularity.sh` script,
you can activate the `/ext3/conda/bootcamp` environment).

```{bash}
./start_singularity.sh
Singularity> conda activate /ext3/conda/bootcamp
Singularity> ./scripts/run_sshd.sh
```

Note that this script attempts to preserve several important environment variables in order to preserve
all functionality including GPU functionality in the image. If running `nvidia-smi` on a new `ssh` shell
fails inside the container, your environment variables were probably not preserved correctly.
