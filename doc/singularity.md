# Singularity on GCP bursting

This document provides more information and background on using singularity on the GCP bursting server.
Singularity is the preferred way of setting up reproducible environments that can be transported between
GCP and Greene.

## Singularity containers

Singularity is accessed through the command line, and is available on both Greene and the GCP machines.
Singularity containers are defined through the `.sif` file format, but can be created from docker containers
and other OCI containers through the `singularity pull` command.

For example, to create the pytorch singularity container at `/scratch/wz2247/singularity/images/pytorch_22.08-py3.sif`,
I ran the following command:
```{bash}
singularity pull docker://nvcr.io/nvidia/pytorch:22.08-py3
```
Note that creating singularity containers can be very IO intensive, and can be particularly
slow on networked filesystems. On GCP machines, the temporary directory may not be large enough
to create the image, and you may need to run:
```{bash}
SINGULARITY_TMPDIR=/dev/shm singularity pull docker://nvcr.io/nvidia/pytorch:22.08-py3
```
On the standard Greene compute nodes, the HPC team has already set the `SINGULARITY_TMPDIR`
environment variable to a suitable location for best performance.

## Running singularity containers

Singularity containers may be run through the `singularity exec` command,
or through the `singularity shell` command.
In general, I recommend using at least the `--no-home` argument, as well as
the `--cleanenv` arguments, to increase isolation between your container and the host.
It will also often be useful to bind the `/scratch` directory (`--bind /scratch`),
as well as bind some subdirectory of `$HOME` (e.g. `--bind $HOME/.ssh`).
On GPU machines, you will also need the `--nv` flag in order to pass-through the GPUs.
```{bash}
IMAGE=/scratch/wz2247/singularity/images/pytorch_22.08-py3.sif
singularity exec --no-home --cleanenv --nv --bind /scratch --bind $HOME/.ssh $IMAGE /bin/bash
```

If you use `singularity shell`, specify that you are using the `bash` shell through the argument
`--shell=/bin/bash`. This ensures that startup files (e.g. `.bashrc`) are read by the shell, which
is required for the use of `conda`.

## Overlays

Overlays are a powerful tool to work with and modify singularity containers.
The [documentation](https://sites.google.com/a/nyu.edu/nyu-hpc/services/Training-and-Workshops/tutorials/singularity-on-greene)
from the NYU HPC team gives a good introduction on the workflow for these containers.

As an example, I have created a series of script to convert the base Nvidia pytorch container
to a customizable container with overlays. These scripts can be run in any directory on GCP instances,
and may be found in the [lecture2/scripts](../lecture2/scripts) directory.

Generally, overlays are best suited when you wish to logically modify the "image", which should contain
your software dependencies. It will usually be better to keep your own code, data and training results
in the standard file system and access them through a bind point.


## Starting a container

With the overlays as generated by the previous scripts, I have also included an example script
which starts the singularity instance with recommended binds and parameters, in [start_singularity.sh](../lecture2/start_singularity.sh).
This is best suited for running non-interactive jobs (e.g. training / evaluation).
However, for interactive work, I recomend using a singularity instance instead.

## Singularity instances

Singularity instances are designed for longer running applications (e.g. web servers, daemons etc.).
However, they are also great when we wish to create a logically long running instance, e.g. for interactive
work inside a container.

Singularity instances are managed through the `singularity instance` set of commands.
In particular, you will encounter the following commands:
- `singularity instance start`: starts a new instance with the given configuration.
    It is here that you must specify all options for the container (e.g. bind paths,
    overlays etc.), the image that you wish to run, and a name of your choice for
    this container instance.
- `singularity instance list`: lists the set of all container instances which are
    currently running on your system.
- `singularity instance stop`: stops the instance with the given name.

Once your instance is running, you may refer other singularity commands to it.
For example, to run a shell in your given instance, you may use:
```{bash}
singularity shell instance://myinstance
```
As you have already specified all options during the instance creation, there is
no need to specify more options again when running commands using a singularity instance.

## Connecting VSCode to a singularity instance

To be able to access your environment in an ergonomic fashion, we would like VSCode
to connect to the singularity image we are using to develop our software. This will
enable features like auto-completion, tests, debugging etc. using the exact dependencies
that our software expects.

The first step will be to ensure that we can directly connect to a shell *inside* the
container instance. Although one possibility is to run a SSH *server* inside the instance,
this is somewhat complex and prone to errors. Instead, we will make use of the `RemoteCommand`
option to drop our session directly the container.

For example, suppose that we have a singularity instance called `myinstance` running
on a a host with nickname `mycomputer`. We modify the `.ssh/config` file in the following fashion:
```
Host mycomputer mycomputerinstance
    # Configuration from mycomputer here

Host mycomputerinstance
    RemoteCommand singularity instance shell --shell='/bin/bash' instance://myinstance
    RequestTTY true
```
With everything setup correctly, running `ssh mycomputerinstance` should land us in a shell
which is directly running inside the container.

Finally, we need to let VSCode know to explicitly respect the `RemoteCommand` configuration.
You need to set the following two options in your VSCode configuration:
```
{
    remote.SSH.enableLocalServer: true,
    remote.SSH.enableRemoteCommand: true
}
```
With those in place, you should be able to select `mycomputerinstance` from the VSCode SSH remote
targets list, and directly connect into your container!
