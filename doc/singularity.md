# Singularity on GCP bursting

This document provides more information and background on using singularity on the GCP bursting server.
Singularity is the preferred way of setting up reproducible environments that can be transported between
GCP and Greene.

## Singularity containers

Singularity is accessed through the command line, and is available on both Greene and the GCP machines.
Singularity containers are defined through the `.sif` file format, but can be created from docker containers
and other OCI containers through the `singularity pull` command.

For example, to create the pytorch singularity container at `/scratch/wz2247/singularity/images/pytorch_22.08-py3.sif`,
I ran the following command:
```{bash}
singularity pull docker://nvcr.io/nvidia/pytorch:22.08-py3
```
Note that creating singularity containers can be very IO intensive, and can be particularly
slow on networked filesystems. On GCP machines, the temporary directory may not be large enough
to create the image, and you may need to run:
```{bash}
SINGULARITY_TMPDIR=/dev/shm singularity pull docker://nvcr.io/nvidia/pytorch:22.08-py3
```
On the standard Greene compute nodes, the HPC team has already set the `SINGULARITY_TMPDIR`
environment variable to a suitable location for best performance.

## Running singularity containers

Singularity containers may be run through the `singularity exec` command,
or through the `singularity shell` command.
In general, I recommend using at least the `--no-home` argument, as well as
the `--cleanenv` arguments, to increase isolation between your container and the host.
It will also often be useful to bind the `/scratch` directory (`--bind /scratch`),
as well as bind some subdirectory of `$HOME` (e.g. `--bind $HOME/.ssh`).
On GPU machines, you will also need the `--nv` flag in order to pass-through the GPUs.
```{bash}
IMAGE=/scratch/wz2247/singularity/images/pytorch_22.08-py3.sif
singularity exec --no-home --cleanenv --nv --bind /scratch --bind $HOME/.ssh $IMAGE /bin/bash
```

If you use `singularity shell`, specify that you are using the `bash` shell through the argument
`--shell=/bin/bash`. This ensures that startup files (e.g. `.bashrc`) are read by the shell, which
is required for the use of `conda`.

## Overlays

Overlays are a powerful tool to work with and modify singularity containers.
The [documentation](https://sites.google.com/a/nyu.edu/nyu-hpc/services/Training-and-Workshops/tutorials/singularity-on-greene)
from the NYU HPC team gives a good introduction on the workflow for these containers.

As an example, I have created a series of script to convert the base Nvidia pytorch container
to a customizable container with overlays. These scripts can be run in any directory on GCP instances,
and may be found in the [lecture2/scripts](../lecture2/scripts) directory.

Generally, overlays are best suited when you wish to logically modify the "image", which should contain
your software dependencies. It will usually be better to keep your own code, data and training results
in the standard file system and access them through a bind point.


## Starting a container

With the overlays as generated by the previous scripts, I have also included an example script
which starts the singularity instance with recommended binds and parameters, in [start_singularity.sh](../lecture2/start_singularity.sh).
This is best suited for running non-interactive jobs (e.g. training / evaluation).
However, for interactive work, I recomend using a singularity instance instead.

## Singularity instances

Singularity instances are designed for longer running applications (e.g. web servers, daemons etc.).
However, they are also great when we wish to create a logically long running instance, e.g. for interactive
work inside a container.

Singularity instances are managed through the `singularity instance` set of commands.

